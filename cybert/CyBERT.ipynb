{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CyBERT",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvuOVKg9zhLq"
      },
      "source": [
        "# **CyBERT** (**Cy**ber Security **BERT**): Pre-Training and Fine-Tuning\n",
        "\n",
        "This Colab allows you to further pre-train and/or fine-tune a BERT model. The two steps can be performed independently.\n",
        "\n",
        "* Executing the prerequisites is required to be able to perform the subsequent steps. Activating the GPU is optional but highly recommended for accelerating the training.\n",
        "\n",
        "* Part 1 is the Pre-Training step consisting of the creation of a corpus using different text files as source and the subsequent training with the specified Hyperparameters.\n",
        "\n",
        "* Part 2 is the Fine-Tuning. This step can be executed on the pre-trained model from Part 1 or any other BERT model. We provide an already pre-trained CyBERT model in the PEASEC cloud which comes along with the other downloaded files in the prerequisites.\n",
        "Part 2 consists of the pre-processing of a labeled CSV file and the subsequent split into different sets of data. Afterwards the fine-tuning using the specified Hyperparameters can be performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DeERSpE92Zd"
      },
      "source": [
        "# Part 0: Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ORhiisMFJ8b"
      },
      "source": [
        "## Activate GPU\n",
        "\n",
        "For training the model activate the GPU in the notebook settings as follows:\n",
        "\n",
        "*   Edit (Bearbeiten) -> Notebook Settings (Notebook-Einstellungen)\n",
        "*   In the Hardware Accelerator (Hardwarebeschleuniger) dropdown select 'GPU'\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Run the following cell to check whether GPU is enabled.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3LA7ofcEYzp"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgybQwYfACPt"
      },
      "source": [
        "## Downloading and unzipping files\n",
        "\n",
        "In this step all required files are downloaded from the PEASEC Cloud and unzipped. This includes datasets, code and a ready to go CyBERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFzrrB3Z99BG"
      },
      "source": [
        "!wget -O cybert_data.zip https://cloud.peasec.de/index.php/s/YCiyap7yK5DfyFx/download\n",
        "!unzip cybert_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlYiq_UhARZL"
      },
      "source": [
        "## Installing required libraries via pip\n",
        "\n",
        "We have a list of required Python packages and clone the current master branch of the ðŸ¤— Transformer repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InkVQN4OALlV"
      },
      "source": [
        "!pip install -r cybert/requirements.txt\n",
        "\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdY0Ntwxhzpc"
      },
      "source": [
        "# Part 1: Pre-Training CyBERT\n",
        "\n",
        "If you do not want to pre-train CyBERT yourself, you can jump to [Part 2: Fine-Tuning](#fine_tuning) and use the provided CyBERT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbD62fGm2PHG"
      },
      "source": [
        "## Compiling together datasets\n",
        "\n",
        "\n",
        "When running the following cell a *cysec_corpus.txt* file will be compiled from all txt files in *cybert/input/Corpus/* directory.\n",
        "This file is used afterwards for pre-training the model.\n",
        "\n",
        "\n",
        "\n",
        " You can append additional text datasets to the input corpus by saving them as .txt files in *cybert/input* directory before this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm1oRrzlcW75"
      },
      "source": [
        "import glob\n",
        "import shutil\n",
        "\n",
        "input_txt_paths = glob.glob(\"cybert/input/Corpus/*.txt\")\n",
        "\n",
        "with open('cysec_corpus.txt','wb') as output_corpus:\n",
        "    for input_txt_file in input_txt_paths:\n",
        "        with open(input_txt_file,'rb') as input_txt:\n",
        "            shutil.copyfileobj(input_txt, output_corpus)\n",
        "            output_corpus.write(b'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro_eHz_FO4OB"
      },
      "source": [
        "## Hyperparameters and Training the model\n",
        "\n",
        "This step consists of specifying Hyperparameters **and** the training itself. You can modify the parameters as needed.\n",
        "\n",
        "* A list of available pretrained models for using as the pre-training base can be found here:  [HuggingFace Pretrained Models](https://huggingface.co/models)\n",
        "\n",
        "* The full specification of the parameters: [HuggingFace TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments)<br>\n",
        "You can extend the list of inputted parameters by appending arguments to the Python call in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ0h3oFH-C19",
        "cellView": "form"
      },
      "source": [
        "MODEL = 'bert-base-cased' #@param [\"bert-base-cased\"] {allow-input: true}\n",
        "INPUT_FILE_PATH = 'cysec_corpus.txt'  #@param {type: \"string\"}\n",
        "\n",
        "NUM_EPOCHS = 3 #@param {type: \"integer\"}\n",
        "BATCH_SIZE = 4 #@param {type: \"integer\"}\n",
        "LEARN_RATE = \"2e-5\" #@param [\"5e-5\", \"2e-5\", \"1e-5\", \"1e-4\"]\n",
        "WARMUP_STEPS = 10000 #@param {type: \"integer\"}\n",
        "WEIGHT_DECAY = 0.01 #@param {type: \"number\"}\n",
        "\n",
        "SAVE_STRATEGY = \"steps\" #@param [\"no\", \"epoch\", \"steps\"]\n",
        "SAVE_STEPS = 5000 #@param {type: \"integer\"}\n",
        "LOGGING_STEPS = 500 #@param {type: \"integer\"}\n",
        "\n",
        "SEED = 42 #@param {type: \"integer\"}\n",
        "GRADIENT_ACC_STEPS = 1 #@param {type: \"integer\"}\n",
        "\n",
        "OVERWRITE_OUTPUT_DIR = False #@param {type:\"boolean\"}\n",
        "\n",
        "OUTPUT_DIR = 'model/' + 'cybert_e-' + str(NUM_EPOCHS) + '_b-' + str(BATCH_SIZE) + '_l-' + str(LEARN_RATE)\n",
        "\n",
        "\n",
        "!python cybert/code/run_mlm.py \\\n",
        "    --model_name_or_path $MODEL \\\n",
        "    --train_file $INPUT_FILE_PATH \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs $NUM_EPOCHS \\\n",
        "    --per_device_train_batch_size $BATCH_SIZE \\\n",
        "    --learning_rate $LEARN_RATE \\\n",
        "    --weight_decay $WEIGHT_DECAY \\\n",
        "    --output_dir $OUTPUT_DIR \\\n",
        "    --overwrite_output_dir $OVERWRITE_OUTPUT_DIR \\\n",
        "    --save_steps $SAVE_STEPS \\\n",
        "    --save_strategy $SAVE_STRATEGY \\\n",
        "    --warmup_steps $WARMUP_STEPS \\\n",
        "    --cache_dir \"cache\" \\\n",
        "    --gradient_accumulation_steps $GRADIENT_ACC_STEPS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP1fIvErzmqg"
      },
      "source": [
        "<a name=\"fine_tuning\"></a>\n",
        "# Part 2: Fine-Tuning and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch8O5eJH0ELh"
      },
      "source": [
        "## Pre-process and split NER dataset\n",
        "\n",
        "In this step the provided CSV file is formatted into a JSONL and splitted into a Train, a Test and an optional Dev dataset. You can specify the file path if using an own dataset but be aware that the implementation of this pre-processing stepwas developed for the provided dataset and only works on data with the same format. Default is the provided labeled CVE database from the Ovana paper.\n",
        "\n",
        "Other parameters to be specified:\n",
        "* SPLIT_SEED: Choose a seed for the data's random split\n",
        "* SPLIT_RATIO: The train set's size, the remainder is the test set's size or is splitted between test set and dev set equally if CREATE_DEV_SET is true\n",
        "* CREATE_DEV_SET: Decide, whether you want to create a dev set or not - it is not included in the fine-tuning step by default\n",
        "* NER_TAG: Since multi-tagging is not supported, you need to choose the specific tag you want the model to be fine-tuned on\n",
        "* CSV_SEPERATOR: Delimiter of your input CSV file\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDQkAlrY0Dr6",
        "cellView": "form"
      },
      "source": [
        "# Parameters\n",
        "# =============================================================================\n",
        "FILE_PATH = 'cybert/input/NER/tagged_all.csv' #@param {type:\"string\"}\n",
        "SPLIT_SEED =  2021#@param {type: \"integer\"}\n",
        "SPLIT_RATIO = 80 #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "CREATE_DEV_SET = False #@param {type:\"boolean\"}\n",
        "\n",
        "NER_TAG = 'SN' #@param ['SV', 'SN', 'AC']\n",
        "\n",
        "CSV_SEPARATOR = 'Space(s)' #@param ['Space(s)', ';', ',']\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "# Code\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from numpy.random import randn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "O_TAG = 'O'\n",
        "\n",
        "\n",
        "def main():\n",
        "    df = read_csv()\n",
        "\n",
        "    json_list = create_json_per_cve(df)\n",
        "\n",
        "    split_dict = split_data(json_list)\n",
        "\n",
        "    data_to_json(split_dict)\n",
        "\n",
        "\n",
        "def read_csv():\n",
        "    global csv_filename, req_tag\n",
        "    csv_filename = FILE_PATH\n",
        "    req_tag = NER_TAG\n",
        "    csv_sep = '\\s+' if CSV_SEPARATOR.startswith('Space') else CSV_SEPARATOR\n",
        "              #'\\t' if CSV_SEPARATOR == 'tab' else \\\n",
        "              \n",
        "\n",
        "    if(csv_filename[-3:] != 'csv'):\n",
        "        raise ValueError(\"Not a CSV file!\")\n",
        "\n",
        "    df = pd.read_csv(\n",
        "    csv_filename,\n",
        "    sep=csv_sep,\n",
        "    dtype=str,\n",
        "    header=None,\n",
        "    skip_blank_lines=True,\n",
        "    na_filter=True\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_json_per_cve(df):\n",
        "    json_list = []\n",
        "\n",
        "    groupedby_cve = df.groupby(by=2).groups\n",
        "    for cve in groupedby_cve.values():\n",
        "\n",
        "        entities = []\n",
        "        ner = []\n",
        "        for cve_loc in cve:\n",
        "          # workaround to filter out NaN values,\n",
        "          # since pandas.DataFrame.dropna seems to have a bug on Google Colab\n",
        "            if (df.loc[[cve_loc]][0].notna().all() and\n",
        "                df.loc[[cve_loc]][1].notna().all()):\n",
        "                entities.append(df.loc[[cve_loc]][0].values[0])\n",
        "                ner.append(tags_to_tag(df.loc[[cve_loc]][1].values[0]))\n",
        "\n",
        "        json_list.append({\n",
        "            'words' : entities,\n",
        "            'ner': ner\n",
        "             }\n",
        "            )\n",
        "    return json_list\n",
        "    \n",
        "\n",
        "def split_data(json_list):\n",
        "    ret_dict = dict()\n",
        "    seed = SPLIT_SEED\n",
        "    ratio = int(SPLIT_RATIO) / 100\n",
        "\n",
        "    train, dev_test = train_test_split(json_list, train_size=ratio, random_state=seed)\n",
        "    ret_dict = {'train': train,\n",
        "                'test': dev_test,\n",
        "                'dev': None}\n",
        "    if CREATE_DEV_SET:\n",
        "        dev, test = train_test_split(json_list, train_size=0.5, random_state=seed)\n",
        "        ret_dict['dev'] = dev\n",
        "        ret_dict['test'] = test\n",
        "\n",
        "    return ret_dict\n",
        "\n",
        "\n",
        "def data_to_json(split_dict):\n",
        "\n",
        "    json_filename_prefix = Path(csv_filename[:-4] + '_' + req_tag)\n",
        "    json_filename_train = json_filename_prefix / 'train.json'\n",
        "    json_filename_test = json_filename_prefix / 'test.json'\n",
        "    \n",
        "    os.makedirs(os.path.dirname(json_filename_train), exist_ok=True)\n",
        "    with open(json_filename_train, 'w') as json_file:\n",
        "        for e in split_dict['train']:\n",
        "            json.dump(e, json_file)\n",
        "            json_file.write('\\n')\n",
        "        print(\"Train dataset saved in : \" + str(json_filename_train))\n",
        "\n",
        "    with open(json_filename_test, 'w') as json_file:\n",
        "        for e in split_dict['test']:\n",
        "            json.dump(e, json_file)\n",
        "            json_file.write('\\n')\n",
        "        print(\"Test dataset saved in : \" + str(json_filename_test))\n",
        "\n",
        "    if CREATE_DEV_SET:\n",
        "        json_filename_dev = json_filename_prefix / 'dev.json'\n",
        "        with open(json_filename_dev, 'w') as json_file:\n",
        "            for e in split_dict['dev']:\n",
        "                json.dump(e, json_file)\n",
        "                json_file.write('\\n')\n",
        "        print(\"Dev dataset saved in : \" + str(json_filename_dev))\n",
        "\n",
        "\n",
        "def tags_to_tag(tags):\n",
        "    return req_tag if req_tag in str(tags) else O_TAG\n",
        "\n",
        "\n",
        "        \n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrkNk1Sxfiu2"
      },
      "source": [
        "## Fine-Tuning Process\n",
        "\n",
        "* The full specification of the parameters: [HuggingFace TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments)<br>\n",
        "You can extend the list of inputted parameters by appending arguments to the Python call in the following cell.\n",
        "\n",
        "The evaluation results will be printed in the cell's output and also stored in the *all_results.json* file of the fine-tuned model's directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAGTSM5FWXbn",
        "cellView": "form"
      },
      "source": [
        "# ============================== Hyperparameters ==============================\n",
        "\n",
        "MODEL_NAME_OR_PATH = 'cybert/model/CyBERT_beta' #@param {type: \"string\"}\n",
        "DATASET_DIR_PATH = 'cybert/input/NER/tagged_all_SN' #@param {type: \"string\"}\n",
        "NUM_EPOCHS =  4 #@param {type: \"integer\"}\n",
        "BATCH_SIZE =  16 #@param {type: \"integer\"}\n",
        "\n",
        "SAVE_STRATEGY = \"epoch\" #@param [\"no\", \"epoch\", \"steps\"]\n",
        "EVAL_STRATEGY = \"epoch\" #@param [\"no\", \"epoch\", \"steps\"]\n",
        "SAVE_STEPS = 500 #@param {type: \"integer\"}\n",
        "LOGGING_STEPS = 500 #@param {type: \"integer\"}\n",
        "NER_SEED = 42 #@param {type: \"integer\"}\n",
        "\n",
        "OVERWRITE_OUTPUT_DIR = True #@param {type:\"boolean\"}\n",
        "DO_TRAIN = True #@param {type:\"boolean\"}\n",
        "DO_EVAL = True #@param {type:\"boolean\"}\n",
        "\n",
        "# =============================================================================\n",
        "\n",
        "from pathlib import Path\n",
        "datasets_posix = Path(DATASET_DIR_PATH)\n",
        "\n",
        "OUTPUT_DIR = Path('cybert/model') / '{}-fine_tuned-{}'.format(\n",
        "    Path(MODEL_NAME_OR_PATH).stem,\n",
        "    datasets_posix.stem\n",
        "    )\n",
        "print(\"Fine-tuned model will be saved in: \" + str(OUTPUT_DIR))\n",
        "\n",
        "TRAIN_FILE = str(datasets_posix / \"train.json\")\n",
        "TEST_FILE = str(datasets_posix / \"test.json\")\n",
        "DEV_FILE = datasets_posix / \"dev.json\"\n",
        "\n",
        "\n",
        "!python cybert/code/run_ner.py \\\n",
        "  --model_name_or_path=$MODEL_NAME_OR_PATH \\\n",
        "  --task_name=ner \\\n",
        "  --train_file=$TRAIN_FILE \\\n",
        "  --do_train=$DO_TRAIN \\\n",
        "  --validation_file=$TEST_FILE \\\n",
        "  --do_eval=$DO_EVAL \\\n",
        "  --evaluation_strategy=$EVAL_STRATEGY \\\n",
        "  --num_train_epochs=$NUM_EPOCHS \\\n",
        "  --per_device_train_batch_size=$BATCH_SIZE \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --overwrite_output_dir=$OVERWRITE_OUTPUT_DIR \\\n",
        "  --save_strategy=$SAVE_STRATEGY\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}