Downloaded and generated configs for 'superglue_broadcoverage_diagnostics' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_superglue_broadcoverage_diagnostics_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_superglue_broadcoverage_diagnostics_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: False
  do_val: False
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 3854316912
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_superglue_broadcoverage_diagnostics_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_superglue_broadcoverage_diagnostics_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": false,
  "do_val": false,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 3854316912,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    superglue_broadcoverage_diagnostics (RteTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/superglue_broadcoverage_diagnostics_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.superglue_broadcoverage_diagnostics.head.dense.bias
  taskmodels_dict.superglue_broadcoverage_diagnostics.head.out_proj.bias
Using AdamW
Downloaded and generated configs for 'record' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_record_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_record_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 3421571567
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_record_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_record_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 3421571567,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    record (ReCoRDTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/record_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.record.head.dense.bias
  taskmodels_dict.record.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6159805952380953,
  "record": {
    "loss": 0.4366727305654384,
    "metrics": {
      "major": 0.6159805952380953,
      "minor": {
        "em": 0.6125,
        "f1": 0.6194611904761906,
        "f1_em": 0.6159805952380953
      }
    }
  }
}
Downloaded and generated configs for 'rte' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_rte_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_rte_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1024894951
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_rte_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_rte_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1024894951,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    rte (RteTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/rte_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.rte.head.dense.bias
  taskmodels_dict.rte.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.5631768953068592,
  "rte": {
    "loss": 0.6896785895029703,
    "metrics": {
      "major": 0.5631768953068592,
      "minor": {
        "acc": 0.5631768953068592
      }
    }
  }
}
Downloaded and generated configs for 'wic' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_wic_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_wic_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 785290649
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_wic_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_wic_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 785290649,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    wic (WiCTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/wic_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.wic.head.span_attention_extractor._global_attention._module.bias
  taskmodels_dict.wic.head.classifier.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6081504702194357,
  "wic": {
    "loss": 0.6998690962791443,
    "metrics": {
      "major": 0.6081504702194357,
      "minor": {
        "acc": 0.6081504702194357
      }
    }
  }
}
Downloaded and generated configs for 'wsc' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_wsc_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_wsc_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 9578372
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_wsc_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_wsc_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 9578372,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    wsc (WSCTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/wsc_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.wsc.head.span_attention_extractor._global_attention._module.bias
  taskmodels_dict.wsc.head.classifier.bias
Using AdamW
Loading Best
{
  "aggregated": 0.5769230769230769,
  "wsc": {
    "loss": 0.6746501922607422,
    "metrics": {
      "major": 0.5769230769230769,
      "minor": {
        "acc": 0.5769230769230769
      }
    }
  }
}
Downloaded and generated configs for 'superglue_winogender_diagnostics' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_superglue_winogender_diagnostics_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_superglue_winogender_diagnostics_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: False
  do_val: False
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1114426533
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_superglue_winogender_diagnostics_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_superglue_winogender_diagnostics_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": false,
  "do_val": false,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1114426533,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    superglue_winogender_diagnostics (SuperglueWinogenderDiagnosticsTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/superglue_winogender_diagnostics_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.superglue_winogender_diagnostics.head.dense.bias
  taskmodels_dict.superglue_winogender_diagnostics.head.out_proj.bias
Using AdamW
Downloaded and generated configs for 'boolq' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_boolq_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_boolq_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 334661192
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_boolq_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_boolq_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 334661192,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    boolq (BoolQTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/boolq_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.boolq.head.dense.bias
  taskmodels_dict.boolq.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6755351681957187,
  "boolq": {
    "loss": 0.5983265065229856,
    "metrics": {
      "major": 0.6755351681957187,
      "minor": {
        "acc": 0.6755351681957187
      }
    }
  }
}
Downloaded and generated configs for 'cb' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_cb_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_cb_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 3841364417
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_cb_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_cb_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 3841364417,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    cb (CommitmentBankTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/cb_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.cb.head.dense.bias
  taskmodels_dict.cb.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6215087808172914,
  "cb": {
    "loss": 0.9238044023513794,
    "metrics": {
      "major": 0.6215087808172914,
      "minor": {
        "acc": 0.7321428571428571,
        "avg_f1": 0.5108747044917258,
        "f11": 0.7659574468085107,
        "f12": 0.7666666666666666,
        "f13": 0.0
      }
    }
  }
}
Downloaded and generated configs for 'copa' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_copa_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_copa_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 817218615
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_copa_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_copa_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 817218615,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    copa (CopaTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/copa_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.copa.head.dense.bias
  taskmodels_dict.copa.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.49,
  "copa": {
    "loss": 0.6928980350494385,
    "metrics": {
      "major": 0.49,
      "minor": {
        "acc": 0.49
      }
    }
  }
}
Downloaded and generated configs for 'multirc' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_multirc_cybert_v100_r_3_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_multirc_cybert_v100_r_3
  hf_pretrained_model_name_or_path: /work/projects/project01762/CyBERT/model/cybert_v100_r
  model_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 4038429450
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/run_configs/simple_multirc_cybert_v100_r_3_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/runs/simple_multirc_cybert_v100_r_3",
  "hf_pretrained_model_name_or_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r",
  "model_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/model/cybert_v100_r/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 4038429450,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    multirc (MultiRCTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_3/tasks/configs/multirc_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.multirc.head.dense.bias
  taskmodels_dict.multirc.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.3965894409894706,
  "multirc": {
    "loss": 0.602036819646233,
    "metrics": {
      "major": 0.3965894409894706,
      "minor": {
        "em": 0.155299055613851,
        "f1": 0.6378798263650902
      }
    }
  }
}
