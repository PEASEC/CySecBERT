Lmod: loading gcc 8.5.0 
Lmod: loading python 3.10.10 
Lmod: loading cuda 11.8 
Lmod: loading cuDNN 8.8.1 
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29400 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29400 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29400 (errno: 97 - Address family not supported by protocol).
wandb: Currently logged in as: bayer (few_shot). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/mb14sola/.netrc
wandb: Currently logged in as: bayer (few_shot). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/mb14sola/.netrc
2023-04-19 10:21:03.994687: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-19 10:21:03.994690: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-19 10:21:04.339408: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-19 10:21:04.339406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-19 10:21:08.778053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-19 10:21:08.778055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /work/projects/project02060/CyBERT/wandb/run-20230419_102118-3web19zv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-microwave-566
wandb: ‚≠êÔ∏è View project at https://wandb.ai/few_shot/CyBERT
wandb: üöÄ View run at https://wandb.ai/few_shot/CyBERT/runs/3web19zv
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /work/projects/project02060/CyBERT/wandb/run-20230419_102118-4lzubmm0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-bee-566
wandb: ‚≠êÔ∏è View project at https://wandb.ai/few_shot/CyBERT
wandb: üöÄ View run at https://wandb.ai/few_shot/CyBERT/runs/4lzubmm0
[W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:36435 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [gvqc0001]:36435 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [gvqc0001]:36435 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [gvqc0001]:36435 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [gvqc0001]:36435 (errno: 97 - Address family not supported by protocol).
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 187.68it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 507.17it/s]
[INFO|configuration_utils.py:668] 2023-04-19 10:21:19,823 >> loading configuration file config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json
[INFO|configuration_utils.py:720] 2023-04-19 10:21:19,824 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.4",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|configuration_utils.py:668] 2023-04-19 10:21:19,945 >> loading configuration file config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json
[INFO|configuration_utils.py:720] 2023-04-19 10:21:19,947 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.4",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1802] 2023-04-19 10:21:19,948 >> loading file vocab.txt from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt
[INFO|tokenization_utils_base.py:1802] 2023-04-19 10:21:19,949 >> loading file tokenizer.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json
[INFO|tokenization_utils_base.py:1802] 2023-04-19 10:21:19,949 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-04-19 10:21:19,950 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-04-19 10:21:19,950 >> loading file tokenizer_config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json
[INFO|configuration_utils.py:668] 2023-04-19 10:21:19,951 >> loading configuration file config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json
[INFO|configuration_utils.py:720] 2023-04-19 10:21:19,952 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.4",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2403] 2023-04-19 10:21:19,988 >> loading weights file pytorch_model.bin from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin
[INFO|configuration_utils.py:575] 2023-04-19 10:21:20,291 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.4"
}

Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3024] 2023-04-19 10:21:22,330 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:3042] 2023-04-19 10:21:22,331 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
[INFO|modeling_utils.py:2692] 2023-04-19 10:21:22,461 >> Generation config file not found, using a generation config created from the model config.
[INFO|trainer.py:2082] 2023-04-19 10:21:25,244 >> Loading model from model/cybert_v100_test/checkpoint-30.
[INFO|trainer.py:738] 2023-04-19 10:21:25,799 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
/home/mb14sola/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/mb14sola/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1740] 2023-04-19 10:21:27,182 >> ***** Running training *****
[INFO|trainer.py:1741] 2023-04-19 10:21:27,183 >>   Num examples = 6
[INFO|trainer.py:1742] 2023-04-19 10:21:27,183 >>   Num Epochs = 30
[INFO|trainer.py:1743] 2023-04-19 10:21:27,183 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1744] 2023-04-19 10:21:27,184 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1745] 2023-04-19 10:21:27,184 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1746] 2023-04-19 10:21:27,184 >>   Total optimization steps = 30
[INFO|trainer.py:1747] 2023-04-19 10:21:27,186 >>   Number of trainable parameters = 109514298
[INFO|trainer.py:1769] 2023-04-19 10:21:27,188 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:1770] 2023-04-19 10:21:27,188 >>   Continuing training from epoch 30
[INFO|trainer.py:1771] 2023-04-19 10:21:27,188 >>   Continuing training from global step 30
[INFO|trainer.py:1774] 2023-04-19 10:21:27,189 >>   Will skip the first 30 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can install the latest version of Accelerate with `pip install -U accelerate`.You can also add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
0it [00:00, ?it/s]Skipping the first batches: : 0it [00:00, ?it/s][INFO|integrations.py:709] 2023-04-19 10:21:27,191 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"

  0%|          | 0/30 [00:00<?, ?it/s][AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:280] 2023-04-19 10:21:27,208 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[INFO|trainer.py:2012] 2023-04-19 10:21:27,375 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                
                                      [ASkipping the first batches: : 0it [00:00, ?it/s]
  0%|          | 0/30 [00:00<?, ?it/s][A  0%|          | 0/30 [00:00<?, ?it/s]
Skipping the first batches: : 0it [00:00, ?it/s]
wandb: Waiting for W&B process to finish... (success).
[INFO|trainer.py:2814] 2023-04-19 10:21:27,390 >> Saving model checkpoint to model/cybert_v100_test
[INFO|configuration_utils.py:457] 2023-04-19 10:21:27,394 >> Configuration saved in model/cybert_v100_test/config.json
[INFO|configuration_utils.py:362] 2023-04-19 10:21:27,399 >> Configuration saved in model/cybert_v100_test/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-19 10:21:27,922 >> Model weights saved in model/cybert_v100_test/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-19 10:21:27,925 >> tokenizer config file saved in model/cybert_v100_test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-19 10:21:27,928 >> Special tokens file saved in model/cybert_v100_test/special_tokens_map.json
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: üöÄ View run curious-bee-566 at: https://wandb.ai/few_shot/CyBERT/runs/4lzubmm0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230419_102118-4lzubmm0/logs
wandb: | 0.036 MB of 0.036 MB uploaded (0.000 MB deduped)wandb: / 0.036 MB of 0.036 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                    train/epoch ‚ñÅ
wandb:              train/global_step ‚ñÅ
wandb:               train/total_flos ‚ñÅ
wandb:               train/train_loss ‚ñÅ
wandb:            train/train_runtime ‚ñÅ
wandb: train/train_samples_per_second ‚ñÅ
wandb:   train/train_steps_per_second ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    train/epoch 30.0
wandb:              train/global_step 30
wandb:               train/total_flos 63169155563520.0
wandb:               train/train_loss 0.0
wandb:            train/train_runtime 0.1892
wandb: train/train_samples_per_second 951.46
wandb:   train/train_steps_per_second 158.577
wandb: 
wandb: üöÄ View run rosy-microwave-566 at: https://wandb.ai/few_shot/CyBERT/runs/3web19zv
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230419_102118-3web19zv/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/shared/apps/.gcc/8.5/python/3.10.10/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/shared/apps/.gcc/8.5/python/3.10.10/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 259, in check_network_status
    self._loop_check_status(
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 215, in _loop_check_status
    local_handle = request()
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 795, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 601, in _deliver_network_status
    return self._deliver_record(record)
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/mb14sola/.local/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
