Lmod: loading gcc 8.5.0 
Lmod: loading python 3.9.5 
Lmod: loading cuda 11.6 
Lmod: loading cuDNN 8.3.1 
wandb: Currently logged in as: bayer (few_shot). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/mb14sola/.netrc
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /work/projects/project01762/CyBERT/wandb/run-20220518_140025-2k9c6xh3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-fire-324
wandb: ‚≠êÔ∏è View project at https://wandb.ai/few_shot/CyBERT
wandb: üöÄ View run at https://wandb.ai/few_shot/CyBERT/runs/2k9c6xh3
Some weights of the model checkpoint at /work/projects/project01762/CyBERT/model/cybert_v100_r were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /work/projects/project01762/CyBERT/model/cybert_v100_r and are newly initialized: ['bert.pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/mb14sola/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 10644
  Num Epochs = 5
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 6655
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0% 0/6655 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/work/projects/project01762/CyBERT/cybert/code/CySecAlert/run_cysecalert_CyBERT.py", line 91, in <module>
    trainer.train()
  File "/home/mb14sola/.local/lib/python3.9/site-packages/transformers/trainer.py", line 1396, in train
    for step, inputs in enumerate(epoch_iterator):
  File "/shared/apps/.gcc/8.5/python/3.9.5/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/shared/apps/.gcc/8.5/python/3.9.5/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 561, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/shared/apps/.gcc/8.5/python/3.9.5/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/shared/apps/.gcc/8.5/python/3.9.5/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/work/projects/project01762/CyBERT/cybert/code/CySecAlert/run_cysecalert_CyBERT.py", line 48, in __getitem__
    item['labels'] = torch.tensor(self.labels[idx])
TypeError: new(): invalid data type 'str'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: | 0.000 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: / 0.000 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: - 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: \ 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: / 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: - 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: \ 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: / 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced prime-fire-324: https://wandb.ai/few_shot/CyBERT/runs/2k9c6xh3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220518_140025-2k9c6xh3/logs
srun: error: gvqc0004: task 0: Exited with exit code 1
