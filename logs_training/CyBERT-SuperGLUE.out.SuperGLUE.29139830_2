Downloaded and generated configs for 'superglue_broadcoverage_diagnostics' (1/1)
Downloading model
Tokenizing Task 'superglue_broadcoverage_diagnostics' for phases 'test'
RteTask
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/superglue_broadcoverage_diagnostics/test.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_superglue_broadcoverage_diagnostics_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_superglue_broadcoverage_diagnostics_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: False
  do_val: False
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1796164540
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_superglue_broadcoverage_diagnostics_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_superglue_broadcoverage_diagnostics_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": false,
  "do_val": false,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1796164540,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    superglue_broadcoverage_diagnostics (RteTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/superglue_broadcoverage_diagnostics_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.superglue_broadcoverage_diagnostics.head.dense.bias
  taskmodels_dict.superglue_broadcoverage_diagnostics.head.out_proj.bias
Using AdamW
Downloaded and generated configs for 'record' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_record_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_record_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1064444423
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_record_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_record_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1064444423,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    record (ReCoRDTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/record_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.record.head.dense.bias
  taskmodels_dict.record.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6436121428571431,
  "record": {
    "loss": 0.4108435786091941,
    "metrics": {
      "major": 0.6436121428571431,
      "minor": {
        "em": 0.6401,
        "f1": 0.6471242857142863,
        "f1_em": 0.6436121428571431
      }
    }
  }
}
Downloaded and generated configs for 'rte' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_rte_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_rte_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 569647529
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_rte_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_rte_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 569647529,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    rte (RteTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/rte_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.rte.head.dense.bias
  taskmodels_dict.rte.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.5884476534296029,
  "rte": {
    "loss": 0.6476355791091919,
    "metrics": {
      "major": 0.5884476534296029,
      "minor": {
        "acc": 0.5884476534296029
      }
    }
  }
}
Downloaded and generated configs for 'wic' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_wic_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_wic_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 270433976
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_wic_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_wic_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 270433976,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    wic (WiCTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/wic_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.wic.head.span_attention_extractor._global_attention._module.bias
  taskmodels_dict.wic.head.classifier.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6489028213166145,
  "wic": {
    "loss": 0.6661142110824585,
    "metrics": {
      "major": 0.6489028213166145,
      "minor": {
        "acc": 0.6489028213166145
      }
    }
  }
}
Downloaded and generated configs for 'wsc' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_wsc_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_wsc_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1217521465
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_wsc_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_wsc_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1217521465,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    wsc (WSCTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/wsc_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.wsc.head.span_attention_extractor._global_attention._module.bias
  taskmodels_dict.wsc.head.classifier.bias
Using AdamW
Loading Best
{
  "aggregated": 0.5865384615384616,
  "wsc": {
    "loss": 0.6790774464607239,
    "metrics": {
      "major": 0.5865384615384616,
      "minor": {
        "acc": 0.5865384615384616
      }
    }
  }
}
Downloaded and generated configs for 'superglue_winogender_diagnostics' (1/1)
Tokenizing Task 'superglue_winogender_diagnostics' for phases 'test'
SuperglueWinogenderDiagnosticsTask
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/superglue_winogender_diagnostics/test.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_superglue_winogender_diagnostics_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_superglue_winogender_diagnostics_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: False
  do_val: False
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1312741048
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_superglue_winogender_diagnostics_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_superglue_winogender_diagnostics_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": false,
  "do_val": false,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1312741048,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    superglue_winogender_diagnostics (SuperglueWinogenderDiagnosticsTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/superglue_winogender_diagnostics_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.superglue_winogender_diagnostics.head.dense.bias
  taskmodels_dict.superglue_winogender_diagnostics.head.out_proj.bias
Using AdamW
Downloaded and generated configs for 'boolq' (1/1)
Tokenizing Task 'boolq' for phases 'train,val,test'
BoolQTask
  [train]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/boolq/train.jsonl
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/boolq/test.jsonl
  [val]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/boolq/val.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_boolq_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_boolq_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1928884966
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_boolq_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_boolq_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1928884966,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    boolq (BoolQTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/boolq_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.boolq.head.dense.bias
  taskmodels_dict.boolq.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6831804281345566,
  "boolq": {
    "loss": 0.607150701376108,
    "metrics": {
      "major": 0.6831804281345566,
      "minor": {
        "acc": 0.6831804281345566
      }
    }
  }
}
Downloaded and generated configs for 'cb' (1/1)
Tokenizing Task 'cb' for phases 'train,val,test'
CommitmentBankTask
  [train]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/cb/train.jsonl
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/cb/test.jsonl
  [val]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/cb/val.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_cb_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_cb_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 2665656838
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_cb_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_cb_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 2665656838,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    cb (CommitmentBankTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/cb_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.cb.head.dense.bias
  taskmodels_dict.cb.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.3611111111111111,
  "cb": {
    "loss": 0.9853726625442505,
    "metrics": {
      "major": 0.3611111111111111,
      "minor": {
        "acc": 0.5,
        "avg_f1": 0.2222222222222222,
        "f11": 0.0,
        "f12": 0.6666666666666666,
        "f13": 0.0
      }
    }
  }
}
Downloaded and generated configs for 'copa' (1/1)
Tokenizing Task 'copa' for phases 'train,val,test'
CopaTask
  [train]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/copa/train.jsonl
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/copa/test.jsonl
  [val]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/copa/val.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_copa_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_copa_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 3579497504
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_copa_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_copa_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 3579497504,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    copa (CopaTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/copa_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.copa.head.dense.bias
  taskmodels_dict.copa.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.56,
  "copa": {
    "loss": 0.6810324192047119,
    "metrics": {
      "major": 0.56,
      "minor": {
        "acc": 0.56
      }
    }
  }
}
Downloaded and generated configs for 'multirc' (1/1)
Tokenizing Task 'multirc' for phases 'train,val,test'
MultiRCTask
  [train]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/multirc/train.jsonl
  [val]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/multirc/val.jsonl
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/data/multirc/test.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_multirc_bert-base-uncased_2_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_multirc_bert-base-uncased_2
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 3091554884
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/run_configs/simple_multirc_bert-base-uncased_2_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/runs/simple_multirc_bert-base-uncased_2",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 3091554884,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    multirc (MultiRCTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_2/tasks/configs/multirc_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.multirc.head.dense.bias
  taskmodels_dict.multirc.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.4099145758188406,
  "multirc": {
    "loss": 0.6373012473708705,
    "metrics": {
      "major": 0.4099145758188406,
      "minor": {
        "em": 0.16684155299055614,
        "f1": 0.6529875986471251
      }
    }
  }
}
