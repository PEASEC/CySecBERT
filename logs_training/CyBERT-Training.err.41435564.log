Lmod: loading gcc 8.5.0 
Lmod: loading python 3.10.10 
Lmod: loading cuda 11.8 
Lmod: loading cuDNN 8.8.1 
[W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
wandb: Currently logged in as: bayer (few_shot). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/mb14sola/.netrc
wandb: Currently logged in as: bayer (few_shot). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/mb14sola/.netrc
2023-04-20 08:09:10.783842: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-20 08:09:10.783845: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-20 08:09:10.991387: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-20 08:09:10.991388: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-20 08:09:13.551545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-20 08:09:13.551549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
wandb: wandb version 0.15.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /work/projects/project02060/CyBERT/wandb/run-20230420_080919-qsf9iany
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sound-587
wandb: ‚≠êÔ∏è View project at https://wandb.ai/few_shot/CyBERT
wandb: üöÄ View run at https://wandb.ai/few_shot/CyBERT/runs/qsf9iany
wandb: wandb version 0.15.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /work/projects/project02060/CyBERT/wandb/run-20230420_080919-2bmo2vkf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-feather-587
wandb: ‚≠êÔ∏è View project at https://wandb.ai/few_shot/CyBERT
wandb: üöÄ View run at https://wandb.ai/few_shot/CyBERT/runs/2bmo2vkf
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 252.49it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 530.45it/s]
[INFO|configuration_utils.py:668] 2023-04-20 08:09:21,197 >> loading configuration file config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json
[INFO|configuration_utils.py:720] 2023-04-20 08:09:21,198 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.4",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|configuration_utils.py:668] 2023-04-20 08:09:21,325 >> loading configuration file config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json
[INFO|configuration_utils.py:720] 2023-04-20 08:09:21,326 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.4",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1802] 2023-04-20 08:09:21,330 >> loading file vocab.txt from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt
[INFO|tokenization_utils_base.py:1802] 2023-04-20 08:09:21,330 >> loading file tokenizer.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json
[INFO|tokenization_utils_base.py:1802] 2023-04-20 08:09:21,330 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-04-20 08:09:21,330 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-04-20 08:09:21,331 >> loading file tokenizer_config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json
[INFO|configuration_utils.py:668] 2023-04-20 08:09:21,331 >> loading configuration file config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json
[INFO|configuration_utils.py:720] 2023-04-20 08:09:21,332 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.4",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2403] 2023-04-20 08:09:21,374 >> loading weights file pytorch_model.bin from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin
[INFO|configuration_utils.py:575] 2023-04-20 08:09:21,680 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.4"
}

Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3024] 2023-04-20 08:09:23,750 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:3042] 2023-04-20 08:09:23,751 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
[INFO|modeling_utils.py:2692] 2023-04-20 08:09:23,883 >> Generation config file not found, using a generation config created from the model config.
/home/mb14sola/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:738] 2023-04-20 08:09:26,869 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
/home/mb14sola/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1740] 2023-04-20 08:09:27,494 >> ***** Running training *****
[INFO|trainer.py:1741] 2023-04-20 08:09:27,494 >>   Num examples = 6
[INFO|trainer.py:1742] 2023-04-20 08:09:27,495 >>   Num Epochs = 30
[INFO|trainer.py:1743] 2023-04-20 08:09:27,495 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1744] 2023-04-20 08:09:27,495 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1745] 2023-04-20 08:09:27,495 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1746] 2023-04-20 08:09:27,495 >>   Total optimization steps = 30
[INFO|trainer.py:1747] 2023-04-20 08:09:27,496 >>   Number of trainable parameters = 109514298
[INFO|integrations.py:709] 2023-04-20 08:09:27,498 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/30 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-04-20 08:09:27,512 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|‚ñé         | 1/30 [00:02<01:00,  2.08s/it][INFO|trainer.py:2814] 2023-04-20 08:09:29,586 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-1
[INFO|configuration_utils.py:457] 2023-04-20 08:09:29,589 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-1/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:29,593 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-1/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:30,124 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-1/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:30,127 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:30,129 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-1/special_tokens_map.json
  7%|‚ñã         | 2/30 [00:03<00:52,  1.88s/it][INFO|trainer.py:2814] 2023-04-20 08:09:31,337 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-2
[INFO|configuration_utils.py:457] 2023-04-20 08:09:31,340 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-2/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:31,343 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-2/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:31,871 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-2/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:31,873 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:31,874 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-2/special_tokens_map.json
 10%|‚ñà         | 3/30 [00:05<00:49,  1.82s/it][INFO|trainer.py:2814] 2023-04-20 08:09:33,086 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-3
[INFO|configuration_utils.py:457] 2023-04-20 08:09:33,088 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-3/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:33,091 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-3/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:33,607 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-3/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:33,609 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:33,610 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-3/special_tokens_map.json
 13%|‚ñà‚ñé        | 4/30 [00:07<00:46,  1.77s/it][INFO|trainer.py:2814] 2023-04-20 08:09:34,786 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-4
[INFO|configuration_utils.py:457] 2023-04-20 08:09:34,789 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-4/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:34,793 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-4/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:35,339 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-4/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:35,341 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:35,342 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-4/special_tokens_map.json
 17%|‚ñà‚ñã        | 5/30 [00:09<00:43,  1.76s/it][INFO|trainer.py:2814] 2023-04-20 08:09:36,516 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-5
[INFO|configuration_utils.py:457] 2023-04-20 08:09:36,520 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-5/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:36,523 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-5/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:37,050 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-5/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:37,052 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:37,053 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-5/special_tokens_map.json
 20%|‚ñà‚ñà        | 6/30 [00:10<00:41,  1.74s/it][INFO|trainer.py:2814] 2023-04-20 08:09:38,214 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-6
[INFO|configuration_utils.py:457] 2023-04-20 08:09:38,218 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-6/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:38,221 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-6/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:38,749 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-6/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:38,751 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-6/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:38,753 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-6/special_tokens_map.json
 23%|‚ñà‚ñà‚ñé       | 7/30 [00:12<00:39,  1.72s/it][INFO|trainer.py:2814] 2023-04-20 08:09:39,907 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-7
[INFO|configuration_utils.py:457] 2023-04-20 08:09:39,909 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-7/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:39,912 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-7/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:40,433 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-7/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:40,435 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-7/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:40,437 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-7/special_tokens_map.json
 27%|‚ñà‚ñà‚ñã       | 8/30 [00:14<00:37,  1.71s/it][INFO|trainer.py:2814] 2023-04-20 08:09:41,587 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-8
[INFO|configuration_utils.py:457] 2023-04-20 08:09:41,590 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-8/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:41,593 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-8/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:42,123 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-8/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:42,125 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-8/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:42,126 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-8/special_tokens_map.json
 30%|‚ñà‚ñà‚ñà       | 9/30 [00:15<00:35,  1.71s/it][INFO|trainer.py:2814] 2023-04-20 08:09:43,302 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-9
[INFO|configuration_utils.py:457] 2023-04-20 08:09:43,305 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-9/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:43,308 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-9/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:43,837 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-9/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:43,839 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-9/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:43,840 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-9/special_tokens_map.json
 33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [00:17<00:34,  1.72s/it][INFO|trainer.py:2814] 2023-04-20 08:09:45,034 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-10
[INFO|configuration_utils.py:457] 2023-04-20 08:09:45,037 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-10/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:45,040 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-10/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:45,573 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-10/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:45,576 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:45,577 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-10/special_tokens_map.json
 37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [00:19<00:33,  1.74s/it][INFO|trainer.py:2814] 2023-04-20 08:09:46,835 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-11
[INFO|configuration_utils.py:457] 2023-04-20 08:09:46,838 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-11/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:46,841 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-11/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:47,367 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-11/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:47,370 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-11/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:47,371 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-11/special_tokens_map.json
 40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [00:21<00:31,  1.75s/it][INFO|trainer.py:2814] 2023-04-20 08:09:48,587 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-12
[INFO|configuration_utils.py:457] 2023-04-20 08:09:48,591 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-12/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:48,594 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-12/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:49,111 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-12/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:49,114 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-12/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:49,116 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-12/special_tokens_map.json
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 13/30 [00:22<00:29,  1.73s/it][INFO|trainer.py:2814] 2023-04-20 08:09:50,283 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-13
[INFO|configuration_utils.py:457] 2023-04-20 08:09:50,286 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-13/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:50,289 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-13/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:51,291 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-13/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:51,293 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-13/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:51,294 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-13/special_tokens_map.json
 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [00:25<00:30,  1.89s/it][INFO|trainer.py:2814] 2023-04-20 08:09:52,535 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-14
[INFO|configuration_utils.py:457] 2023-04-20 08:09:52,538 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-14/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:52,541 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-14/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:53,058 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-14/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:53,060 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-14/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:53,061 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-14/special_tokens_map.json
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [00:26<00:27,  1.85s/it][INFO|trainer.py:2814] 2023-04-20 08:09:54,285 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-15
[INFO|configuration_utils.py:457] 2023-04-20 08:09:54,288 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-15/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:54,292 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-15/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:54,805 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-15/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:54,807 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-15/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:54,809 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-15/special_tokens_map.json
 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 16/30 [00:28<00:25,  1.81s/it][INFO|trainer.py:2814] 2023-04-20 08:09:56,009 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-16
[INFO|configuration_utils.py:457] 2023-04-20 08:09:56,012 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-16/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:56,015 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-16/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:56,545 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-16/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:56,547 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-16/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:56,549 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-16/special_tokens_map.json
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [00:30<00:23,  1.79s/it][INFO|trainer.py:2814] 2023-04-20 08:09:57,740 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-17
[INFO|configuration_utils.py:457] 2023-04-20 08:09:57,744 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-17/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:57,747 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-17/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:09:58,267 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-17/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:09:58,269 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:09:58,271 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-17/special_tokens_map.json
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [00:31<00:21,  1.78s/it][INFO|trainer.py:2814] 2023-04-20 08:09:59,505 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-18
[INFO|configuration_utils.py:457] 2023-04-20 08:09:59,508 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-18/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:09:59,512 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-18/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:00,034 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-18/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:00,037 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-18/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:00,039 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-18/special_tokens_map.json
 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 19/30 [00:33<00:19,  1.76s/it][INFO|trainer.py:2814] 2023-04-20 08:10:01,208 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-19
[INFO|configuration_utils.py:457] 2023-04-20 08:10:01,213 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-19/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:01,216 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-19/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:01,762 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-19/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:01,765 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-19/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:01,766 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-19/special_tokens_map.json
 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [00:35<00:17,  1.75s/it][INFO|trainer.py:2814] 2023-04-20 08:10:02,933 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-20
[INFO|configuration_utils.py:457] 2023-04-20 08:10:02,936 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-20/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:02,939 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-20/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:03,487 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-20/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:03,489 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:03,491 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-20/special_tokens_map.json
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [00:37<00:15,  1.75s/it][INFO|trainer.py:2814] 2023-04-20 08:10:04,694 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-21
[INFO|configuration_utils.py:457] 2023-04-20 08:10:04,698 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-21/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:04,701 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-21/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:05,223 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-21/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:05,226 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-21/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:05,227 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-21/special_tokens_map.json
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 22/30 [00:38<00:13,  1.75s/it][INFO|trainer.py:2814] 2023-04-20 08:10:06,429 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-22
[INFO|configuration_utils.py:457] 2023-04-20 08:10:06,432 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-22/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:06,435 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-22/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:06,970 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-22/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:06,971 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-22/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:06,972 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-22/special_tokens_map.json
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 23/30 [00:40<00:12,  1.75s/it][INFO|trainer.py:2814] 2023-04-20 08:10:08,183 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-23
[INFO|configuration_utils.py:457] 2023-04-20 08:10:08,186 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-23/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:08,189 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-23/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:08,728 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-23/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:08,730 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-23/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:08,732 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-23/special_tokens_map.json
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [00:42<00:10,  1.75s/it][INFO|trainer.py:2814] 2023-04-20 08:10:09,923 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-24
[INFO|configuration_utils.py:457] 2023-04-20 08:10:09,926 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-24/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:09,929 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-24/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:10,452 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-24/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:10,455 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-24/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:10,457 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-24/special_tokens_map.json
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [00:44<00:08,  1.73s/it][INFO|trainer.py:2814] 2023-04-20 08:10:11,627 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-25
[INFO|configuration_utils.py:457] 2023-04-20 08:10:11,630 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-25/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:11,634 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-25/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:12,159 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-25/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:12,163 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-25/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:12,165 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-25/special_tokens_map.json
 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [00:45<00:06,  1.74s/it][INFO|trainer.py:2814] 2023-04-20 08:10:13,383 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-26
[INFO|configuration_utils.py:457] 2023-04-20 08:10:13,387 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-26/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:13,390 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-26/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:13,963 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-26/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:13,966 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-26/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:13,967 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-26/special_tokens_map.json
 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [00:47<00:05,  1.75s/it][INFO|trainer.py:2814] 2023-04-20 08:10:15,145 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-27
[INFO|configuration_utils.py:457] 2023-04-20 08:10:15,148 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-27/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:15,152 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-27/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:15,701 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-27/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:15,704 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-27/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:15,705 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-27/special_tokens_map.json
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [00:49<00:03,  1.75s/it][INFO|trainer.py:2814] 2023-04-20 08:10:16,889 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-28
[INFO|configuration_utils.py:457] 2023-04-20 08:10:16,892 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-28/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:16,895 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-28/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:17,420 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-28/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:17,423 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-28/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:17,424 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-28/special_tokens_map.json
 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [00:51<00:01,  1.74s/it][INFO|trainer.py:2814] 2023-04-20 08:10:18,614 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-29
[INFO|configuration_utils.py:457] 2023-04-20 08:10:18,618 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-29/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:18,621 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-29/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:19,137 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-29/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:19,139 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-29/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:19,141 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-29/special_tokens_map.json
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:52<00:00,  1.74s/it][INFO|trainer.py:2814] 2023-04-20 08:10:20,351 >> Saving model checkpoint to model/cybert_v100_test_1097/checkpoint-30
[INFO|configuration_utils.py:457] 2023-04-20 08:10:20,356 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-30/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:20,359 >> Configuration saved in model/cybert_v100_test_1097/checkpoint-30/generation_config.json
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:20,901 >> Model weights saved in model/cybert_v100_test_1097/checkpoint-30/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:20,947 >> tokenizer config file saved in model/cybert_v100_test_1097/checkpoint-30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:20,949 >> Special tokens file saved in model/cybert_v100_test_1097/checkpoint-30/special_tokens_map.json
[INFO|trainer.py:2012] 2023-04-20 08:10:22,490 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:54<00:00,  1.74s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:54<00:00,  1.83s/it]
[INFO|trainer.py:2814] 2023-04-20 08:10:22,498 >> Saving model checkpoint to model/cybert_v100_test_1097
[INFO|configuration_utils.py:457] 2023-04-20 08:10:22,500 >> Configuration saved in model/cybert_v100_test_1097/config.json
[INFO|configuration_utils.py:362] 2023-04-20 08:10:22,505 >> Configuration saved in model/cybert_v100_test_1097/generation_config.json
wandb: Waiting for W&B process to finish... (success).
[INFO|modeling_utils.py:1762] 2023-04-20 08:10:23,022 >> Model weights saved in model/cybert_v100_test_1097/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-20 08:10:23,024 >> tokenizer config file saved in model/cybert_v100_test_1097/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-20 08:10:23,025 >> Special tokens file saved in model/cybert_v100_test_1097/special_tokens_map.json
wandb: Waiting for W&B process to finish... (success).
wandb: üöÄ View run atomic-feather-587 at: https://wandb.ai/few_shot/CyBERT/runs/2bmo2vkf
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230420_080919-2bmo2vkf/logs
wandb: 
wandb: Run history:
wandb:                    train/epoch ‚ñÅ
wandb:              train/global_step ‚ñÅ
wandb:               train/total_flos ‚ñÅ
wandb:               train/train_loss ‚ñÅ
wandb:            train/train_runtime ‚ñÅ
wandb: train/train_samples_per_second ‚ñÅ
wandb:   train/train_steps_per_second ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    train/epoch 30.0
wandb:              train/global_step 30
wandb:               train/total_flos 47376865689600.0
wandb:               train/train_loss 3.54687
wandb:            train/train_runtime 54.9942
wandb: train/train_samples_per_second 3.273
wandb:   train/train_steps_per_second 0.546
wandb: 
wandb: üöÄ View run elated-sound-587 at: https://wandb.ai/few_shot/CyBERT/runs/qsf9iany
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230420_080919-qsf9iany/logs
