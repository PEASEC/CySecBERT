Lmod: unloading cuDNN 8.8.1 
Lmod: unloading cuda 11.8 
Lmod: unloading python 3.10.10 
Lmod: unloading gcc 8.5.0 
Lmod: loading gcc 8.5.0 
Lmod: loading python 3.10.10 
Lmod: loading cuda 11.8 
Lmod: loading cuDNN 8.8.1 
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29400 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29400 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29400 (errno: 97 - Address family not supported by protocol).
wandb: Currently logged in as: bayer (few_shot). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/mb14sola/.netrc
wandb: Currently logged in as: bayer (few_shot). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/mb14sola/.netrc
2023-04-18 12:55:35.529418: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-18 12:55:35.529421: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-18 12:55:35.778600: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-18 12:55:35.778744: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-18 12:55:40.136470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-18 12:55:40.136473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /work/projects/project02060/CyBERT/wandb/run-20230418_125549-qsjwf22x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-pyramid-560
wandb: ‚≠êÔ∏è View project at https://wandb.ai/few_shot/CyBERT
wandb: üöÄ View run at https://wandb.ai/few_shot/CyBERT/runs/qsjwf22x
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /work/projects/project02060/CyBERT/wandb/run-20230418_125549-ht5jzc7l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-bush-560
wandb: ‚≠êÔ∏è View project at https://wandb.ai/few_shot/CyBERT
wandb: üöÄ View run at https://wandb.ai/few_shot/CyBERT/runs/ht5jzc7l
[W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:50915 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [gaqc0002]:50915 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [gaqc0002]:50915 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [gaqc0002]:50915 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [gaqc0002]:50915 (errno: 97 - Address family not supported by protocol).
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 221.99it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 529.18it/s]
[INFO|configuration_utils.py:668] 2023-04-18 12:55:51,232 >> loading configuration file config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json
[INFO|configuration_utils.py:720] 2023-04-18 12:55:51,233 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.4",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|configuration_utils.py:668] 2023-04-18 12:55:51,360 >> loading configuration file config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json
[INFO|configuration_utils.py:720] 2023-04-18 12:55:51,361 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.4",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1802] 2023-04-18 12:55:51,362 >> loading file vocab.txt from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt
[INFO|tokenization_utils_base.py:1802] 2023-04-18 12:55:51,362 >> loading file tokenizer.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json
[INFO|tokenization_utils_base.py:1802] 2023-04-18 12:55:51,362 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-04-18 12:55:51,363 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-04-18 12:55:51,363 >> loading file tokenizer_config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json
[INFO|configuration_utils.py:668] 2023-04-18 12:55:51,363 >> loading configuration file config.json from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json
[INFO|configuration_utils.py:720] 2023-04-18 12:55:51,364 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.27.4",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2403] 2023-04-18 12:55:51,400 >> loading weights file pytorch_model.bin from cache at cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin
[INFO|configuration_utils.py:575] 2023-04-18 12:55:51,683 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "pad_token_id": 0,
  "transformers_version": "4.27.4"
}

[WARNING|modeling_utils.py:3024] 2023-04-18 12:55:53,732 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:3042] 2023-04-18 12:55:53,733 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:2692] 2023-04-18 12:55:53,891 >> Generation config file not found, using a generation config created from the model config.
[INFO|trainer.py:2082] 2023-04-18 12:55:56,676 >> Loading model from model/cybert_v100_test/checkpoint-30.
[INFO|trainer.py:738] 2023-04-18 12:55:57,253 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.
/home/mb14sola/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/mb14sola/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1740] 2023-04-18 12:56:01,134 >> ***** Running training *****
[INFO|trainer.py:1741] 2023-04-18 12:56:01,135 >>   Num examples = 6
[INFO|trainer.py:1742] 2023-04-18 12:56:01,135 >>   Num Epochs = 30
[INFO|trainer.py:1743] 2023-04-18 12:56:01,136 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1744] 2023-04-18 12:56:01,136 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1745] 2023-04-18 12:56:01,136 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1746] 2023-04-18 12:56:01,137 >>   Total optimization steps = 30
[INFO|trainer.py:1747] 2023-04-18 12:56:01,138 >>   Number of trainable parameters = 109514298
[INFO|trainer.py:1769] 2023-04-18 12:56:01,139 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:1770] 2023-04-18 12:56:01,139 >>   Continuing training from epoch 30
[INFO|trainer.py:1771] 2023-04-18 12:56:01,139 >>   Continuing training from global step 30
[INFO|trainer.py:1774] 2023-04-18 12:56:01,140 >>   Will skip the first 30 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can install the latest version of Accelerate with `pip install -U accelerate`.You can also add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
0it [00:00, ?it/s]Skipping the first batches: : 0it [00:00, ?it/s][INFO|integrations.py:709] 2023-04-18 12:56:01,142 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/30 [00:00<?, ?it/s][A[WARNING|logging.py:280] 2023-04-18 12:56:01,166 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[INFO|trainer.py:2012] 2023-04-18 12:56:01,320 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                
                                      [ASkipping the first batches: : 0it [00:00, ?it/s]
  0%|          | 0/30 [00:00<?, ?it/s][A  0%|          | 0/30 [00:00<?, ?it/s]
Skipping the first batches: : 0it [00:00, ?it/s]
[INFO|trainer.py:2814] 2023-04-18 12:56:01,330 >> Saving model checkpoint to model/cybert_v100_test
[INFO|configuration_utils.py:457] 2023-04-18 12:56:01,332 >> Configuration saved in model/cybert_v100_test/config.json
[INFO|configuration_utils.py:362] 2023-04-18 12:56:01,335 >> Configuration saved in model/cybert_v100_test/generation_config.json
wandb: Waiting for W&B process to finish... (success).
[INFO|modeling_utils.py:1762] 2023-04-18 12:56:01,866 >> Model weights saved in model/cybert_v100_test/pytorch_model.bin
[INFO|tokenization_utils_base.py:2163] 2023-04-18 12:56:01,868 >> tokenizer config file saved in model/cybert_v100_test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2170] 2023-04-18 12:56:01,869 >> Special tokens file saved in model/cybert_v100_test/special_tokens_map.json
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.016 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.018 MB of 0.018 MB uploaded (0.000 MB deduped)wandb: | 0.036 MB of 0.036 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                    train/epoch ‚ñÅ
wandb:              train/global_step ‚ñÅ
wandb:               train/total_flos ‚ñÅ
wandb:               train/train_loss ‚ñÅ
wandb:            train/train_runtime ‚ñÅ
wandb: train/train_samples_per_second ‚ñÅ
wandb:   train/train_steps_per_second ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    train/epoch 30.0
wandb:              train/global_step 30
wandb:               train/total_flos 63169155563520.0
wandb:               train/train_loss 0.0
wandb:            train/train_runtime 0.1817
wandb: train/train_samples_per_second 990.563
wandb:   train/train_steps_per_second 165.094
wandb: 
wandb: üöÄ View run comfy-pyramid-560 at: https://wandb.ai/few_shot/CyBERT/runs/qsjwf22x
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230418_125549-qsjwf22x/logs
wandb: üöÄ View run icy-bush-560 at: https://wandb.ai/few_shot/CyBERT/runs/ht5jzc7l
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230418_125549-ht5jzc7l/logs
