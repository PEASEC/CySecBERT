Downloaded and generated configs for 'superglue_broadcoverage_diagnostics' (1/1)
Downloading model
Tokenizing Task 'superglue_broadcoverage_diagnostics' for phases 'test'
RteTask
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/superglue_broadcoverage_diagnostics/test.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_superglue_broadcoverage_diagnostics_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_superglue_broadcoverage_diagnostics_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: False
  do_val: False
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 688549066
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_superglue_broadcoverage_diagnostics_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_superglue_broadcoverage_diagnostics_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": false,
  "do_val": false,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 688549066,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    superglue_broadcoverage_diagnostics (RteTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/superglue_broadcoverage_diagnostics_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.superglue_broadcoverage_diagnostics.head.dense.bias
  taskmodels_dict.superglue_broadcoverage_diagnostics.head.out_proj.bias
Using AdamW
Downloaded and generated configs for 'record' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_record_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_record_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 4126200921
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_record_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_record_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 4126200921,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    record (ReCoRDTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/record_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.record.head.dense.bias
  taskmodels_dict.record.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6409552380952382,
  "record": {
    "loss": 0.4065081927226515,
    "metrics": {
      "major": 0.6409552380952382,
      "minor": {
        "em": 0.6375,
        "f1": 0.6444104761904765,
        "f1_em": 0.6409552380952382
      }
    }
  }
}
Downloaded and generated configs for 'rte' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_rte_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_rte_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1820981598
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_rte_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_rte_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1820981598,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    rte (RteTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/rte_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.rte.head.dense.bias
  taskmodels_dict.rte.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.5884476534296029,
  "rte": {
    "loss": 0.6563926736513773,
    "metrics": {
      "major": 0.5884476534296029,
      "minor": {
        "acc": 0.5884476534296029
      }
    }
  }
}
Downloaded and generated configs for 'wic' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_wic_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_wic_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 3822860730
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_wic_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_wic_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 3822860730,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    wic (WiCTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/wic_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.wic.head.span_attention_extractor._global_attention._module.bias
  taskmodels_dict.wic.head.classifier.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6598746081504702,
  "wic": {
    "loss": 0.66157466173172,
    "metrics": {
      "major": 0.6598746081504702,
      "minor": {
        "acc": 0.6598746081504702
      }
    }
  }
}
Downloaded and generated configs for 'wsc' (1/1)
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_wsc_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_wsc_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 172212694
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_wsc_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_wsc_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 172212694,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    wsc (WSCTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/wsc_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.wsc.head.span_attention_extractor._global_attention._module.bias
  taskmodels_dict.wsc.head.classifier.bias
Using AdamW
Loading Best
{
  "aggregated": 0.5,
  "wsc": {
    "loss": 0.6960045695304871,
    "metrics": {
      "major": 0.5,
      "minor": {
        "acc": 0.5
      }
    }
  }
}
Downloaded and generated configs for 'superglue_winogender_diagnostics' (1/1)
Tokenizing Task 'superglue_winogender_diagnostics' for phases 'test'
SuperglueWinogenderDiagnosticsTask
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/superglue_winogender_diagnostics/test.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_superglue_winogender_diagnostics_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_superglue_winogender_diagnostics_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: False
  do_val: False
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1778860381
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_superglue_winogender_diagnostics_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_superglue_winogender_diagnostics_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": false,
  "do_val": false,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1778860381,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    superglue_winogender_diagnostics (SuperglueWinogenderDiagnosticsTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/superglue_winogender_diagnostics_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.superglue_winogender_diagnostics.head.dense.bias
  taskmodels_dict.superglue_winogender_diagnostics.head.out_proj.bias
Using AdamW
Downloaded and generated configs for 'boolq' (1/1)
Tokenizing Task 'boolq' for phases 'train,val,test'
BoolQTask
  [train]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/boolq/train.jsonl
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/boolq/test.jsonl
  [val]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/boolq/val.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_boolq_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_boolq_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 2116825723
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_boolq_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_boolq_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 2116825723,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    boolq (BoolQTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/boolq_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.boolq.head.dense.bias
  taskmodels_dict.boolq.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.6614678899082569,
  "boolq": {
    "loss": 0.6202115691625155,
    "metrics": {
      "major": 0.6614678899082569,
      "minor": {
        "acc": 0.6614678899082569
      }
    }
  }
}
Downloaded and generated configs for 'cb' (1/1)
Tokenizing Task 'cb' for phases 'train,val,test'
CommitmentBankTask
  [train]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/cb/train.jsonl
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/cb/test.jsonl
  [val]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/cb/val.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_cb_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_cb_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 3249226829
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_cb_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_cb_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 3249226829,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    cb (CommitmentBankTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/cb_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.cb.head.dense.bias
  taskmodels_dict.cb.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.46622369878183834,
  "cb": {
    "loss": 0.9817588925361633,
    "metrics": {
      "major": 0.46622369878183834,
      "minor": {
        "acc": 0.5535714285714286,
        "avg_f1": 0.37887596899224807,
        "f11": 0.5116279069767442,
        "f12": 0.6250000000000001,
        "f13": 0.0
      }
    }
  }
}
Downloaded and generated configs for 'copa' (1/1)
Tokenizing Task 'copa' for phases 'train,val,test'
CopaTask
  [train]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/copa/train.jsonl
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/copa/test.jsonl
  [val]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/copa/val.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_copa_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_copa_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 3774298970
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_copa_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_copa_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 3774298970,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    copa (CopaTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/copa_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.copa.head.dense.bias
  taskmodels_dict.copa.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.61,
  "copa": {
    "loss": 0.6694120168685913,
    "metrics": {
      "major": 0.61,
      "minor": {
        "acc": 0.61
      }
    }
  }
}
Downloaded and generated configs for 'multirc' (1/1)
Tokenizing Task 'multirc' for phases 'train,val,test'
MultiRCTask
  [train]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/multirc/train.jsonl
  [val]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/multirc/val.jsonl
  [test]: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/data/multirc/test.jsonl
Running from start
  jiant_task_container_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_multirc_bert-base-uncased_5_config.json
  output_dir: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_multirc_bert-base-uncased_5
  hf_pretrained_model_name_or_path: bert-base-uncased
  model_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p
  model_config_path: /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json
  model_load_mode: from_transformers
  do_train: True
  do_val: True
  do_save: False
  do_save_last: False
  do_save_best: False
  write_val_preds: False
  write_test_preds: False
  eval_every_steps: 0
  save_every_steps: 0
  save_checkpoint_every_steps: 0
  no_improvements_for_n_evals: 0
  keep_checkpoint_when_done: False
  force_overwrite: False
  seed: -1
  learning_rate: 1e-05
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  optimizer_type: adam
  no_cuda: False
  fp16: False
  fp16_opt_level: O1
  local_rank: -1
  server_ip: 
  server_port: 
device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Using seed: 1194682627
{
  "jiant_task_container_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/run_configs/simple_multirc_bert-base-uncased_5_config.json",
  "output_dir": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/runs/simple_multirc_bert-base-uncased_5",
  "hf_pretrained_model_name_or_path": "bert-base-uncased",
  "model_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/model.p",
  "model_config_path": "/work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/models/bert-base-uncased/model/config.json",
  "model_load_mode": "from_transformers",
  "do_train": true,
  "do_val": true,
  "do_save": false,
  "do_save_last": false,
  "do_save_best": false,
  "write_val_preds": false,
  "write_test_preds": false,
  "eval_every_steps": 0,
  "save_every_steps": 0,
  "save_checkpoint_every_steps": 0,
  "no_improvements_for_n_evals": 0,
  "keep_checkpoint_when_done": false,
  "force_overwrite": false,
  "seed": 1194682627,
  "learning_rate": 1e-05,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "optimizer_type": "adam",
  "no_cuda": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "local_rank": -1,
  "server_ip": "",
  "server_port": ""
}
1
Creating Tasks:
    multirc (MultiRCTask): /work/projects/project01762/CyBERT/cybert/code/SuperGLUE/exp_5/tasks/configs/multirc_config.json
No optimizer decay for:
  encoder.embeddings.LayerNorm.weight
  encoder.embeddings.LayerNorm.bias
  encoder.encoder.layer.0.attention.self.query.bias
  encoder.encoder.layer.0.attention.self.key.bias
  encoder.encoder.layer.0.attention.self.value.bias
  encoder.encoder.layer.0.attention.output.dense.bias
  encoder.encoder.layer.0.attention.output.LayerNorm.weight
  encoder.encoder.layer.0.attention.output.LayerNorm.bias
  encoder.encoder.layer.0.intermediate.dense.bias
  encoder.encoder.layer.0.output.dense.bias
  encoder.encoder.layer.0.output.LayerNorm.weight
  encoder.encoder.layer.0.output.LayerNorm.bias
  encoder.encoder.layer.1.attention.self.query.bias
  encoder.encoder.layer.1.attention.self.key.bias
  encoder.encoder.layer.1.attention.self.value.bias
  encoder.encoder.layer.1.attention.output.dense.bias
  encoder.encoder.layer.1.attention.output.LayerNorm.weight
  encoder.encoder.layer.1.attention.output.LayerNorm.bias
  encoder.encoder.layer.1.intermediate.dense.bias
  encoder.encoder.layer.1.output.dense.bias
  encoder.encoder.layer.1.output.LayerNorm.weight
  encoder.encoder.layer.1.output.LayerNorm.bias
  encoder.encoder.layer.2.attention.self.query.bias
  encoder.encoder.layer.2.attention.self.key.bias
  encoder.encoder.layer.2.attention.self.value.bias
  encoder.encoder.layer.2.attention.output.dense.bias
  encoder.encoder.layer.2.attention.output.LayerNorm.weight
  encoder.encoder.layer.2.attention.output.LayerNorm.bias
  encoder.encoder.layer.2.intermediate.dense.bias
  encoder.encoder.layer.2.output.dense.bias
  encoder.encoder.layer.2.output.LayerNorm.weight
  encoder.encoder.layer.2.output.LayerNorm.bias
  encoder.encoder.layer.3.attention.self.query.bias
  encoder.encoder.layer.3.attention.self.key.bias
  encoder.encoder.layer.3.attention.self.value.bias
  encoder.encoder.layer.3.attention.output.dense.bias
  encoder.encoder.layer.3.attention.output.LayerNorm.weight
  encoder.encoder.layer.3.attention.output.LayerNorm.bias
  encoder.encoder.layer.3.intermediate.dense.bias
  encoder.encoder.layer.3.output.dense.bias
  encoder.encoder.layer.3.output.LayerNorm.weight
  encoder.encoder.layer.3.output.LayerNorm.bias
  encoder.encoder.layer.4.attention.self.query.bias
  encoder.encoder.layer.4.attention.self.key.bias
  encoder.encoder.layer.4.attention.self.value.bias
  encoder.encoder.layer.4.attention.output.dense.bias
  encoder.encoder.layer.4.attention.output.LayerNorm.weight
  encoder.encoder.layer.4.attention.output.LayerNorm.bias
  encoder.encoder.layer.4.intermediate.dense.bias
  encoder.encoder.layer.4.output.dense.bias
  encoder.encoder.layer.4.output.LayerNorm.weight
  encoder.encoder.layer.4.output.LayerNorm.bias
  encoder.encoder.layer.5.attention.self.query.bias
  encoder.encoder.layer.5.attention.self.key.bias
  encoder.encoder.layer.5.attention.self.value.bias
  encoder.encoder.layer.5.attention.output.dense.bias
  encoder.encoder.layer.5.attention.output.LayerNorm.weight
  encoder.encoder.layer.5.attention.output.LayerNorm.bias
  encoder.encoder.layer.5.intermediate.dense.bias
  encoder.encoder.layer.5.output.dense.bias
  encoder.encoder.layer.5.output.LayerNorm.weight
  encoder.encoder.layer.5.output.LayerNorm.bias
  encoder.encoder.layer.6.attention.self.query.bias
  encoder.encoder.layer.6.attention.self.key.bias
  encoder.encoder.layer.6.attention.self.value.bias
  encoder.encoder.layer.6.attention.output.dense.bias
  encoder.encoder.layer.6.attention.output.LayerNorm.weight
  encoder.encoder.layer.6.attention.output.LayerNorm.bias
  encoder.encoder.layer.6.intermediate.dense.bias
  encoder.encoder.layer.6.output.dense.bias
  encoder.encoder.layer.6.output.LayerNorm.weight
  encoder.encoder.layer.6.output.LayerNorm.bias
  encoder.encoder.layer.7.attention.self.query.bias
  encoder.encoder.layer.7.attention.self.key.bias
  encoder.encoder.layer.7.attention.self.value.bias
  encoder.encoder.layer.7.attention.output.dense.bias
  encoder.encoder.layer.7.attention.output.LayerNorm.weight
  encoder.encoder.layer.7.attention.output.LayerNorm.bias
  encoder.encoder.layer.7.intermediate.dense.bias
  encoder.encoder.layer.7.output.dense.bias
  encoder.encoder.layer.7.output.LayerNorm.weight
  encoder.encoder.layer.7.output.LayerNorm.bias
  encoder.encoder.layer.8.attention.self.query.bias
  encoder.encoder.layer.8.attention.self.key.bias
  encoder.encoder.layer.8.attention.self.value.bias
  encoder.encoder.layer.8.attention.output.dense.bias
  encoder.encoder.layer.8.attention.output.LayerNorm.weight
  encoder.encoder.layer.8.attention.output.LayerNorm.bias
  encoder.encoder.layer.8.intermediate.dense.bias
  encoder.encoder.layer.8.output.dense.bias
  encoder.encoder.layer.8.output.LayerNorm.weight
  encoder.encoder.layer.8.output.LayerNorm.bias
  encoder.encoder.layer.9.attention.self.query.bias
  encoder.encoder.layer.9.attention.self.key.bias
  encoder.encoder.layer.9.attention.self.value.bias
  encoder.encoder.layer.9.attention.output.dense.bias
  encoder.encoder.layer.9.attention.output.LayerNorm.weight
  encoder.encoder.layer.9.attention.output.LayerNorm.bias
  encoder.encoder.layer.9.intermediate.dense.bias
  encoder.encoder.layer.9.output.dense.bias
  encoder.encoder.layer.9.output.LayerNorm.weight
  encoder.encoder.layer.9.output.LayerNorm.bias
  encoder.encoder.layer.10.attention.self.query.bias
  encoder.encoder.layer.10.attention.self.key.bias
  encoder.encoder.layer.10.attention.self.value.bias
  encoder.encoder.layer.10.attention.output.dense.bias
  encoder.encoder.layer.10.attention.output.LayerNorm.weight
  encoder.encoder.layer.10.attention.output.LayerNorm.bias
  encoder.encoder.layer.10.intermediate.dense.bias
  encoder.encoder.layer.10.output.dense.bias
  encoder.encoder.layer.10.output.LayerNorm.weight
  encoder.encoder.layer.10.output.LayerNorm.bias
  encoder.encoder.layer.11.attention.self.query.bias
  encoder.encoder.layer.11.attention.self.key.bias
  encoder.encoder.layer.11.attention.self.value.bias
  encoder.encoder.layer.11.attention.output.dense.bias
  encoder.encoder.layer.11.attention.output.LayerNorm.weight
  encoder.encoder.layer.11.attention.output.LayerNorm.bias
  encoder.encoder.layer.11.intermediate.dense.bias
  encoder.encoder.layer.11.output.dense.bias
  encoder.encoder.layer.11.output.LayerNorm.weight
  encoder.encoder.layer.11.output.LayerNorm.bias
  encoder.pooler.dense.bias
  taskmodels_dict.multirc.head.dense.bias
  taskmodels_dict.multirc.head.out_proj.bias
Using AdamW
Loading Best
{
  "aggregated": 0.4059944016063447,
  "multirc": {
    "loss": 0.622648832829375,
    "metrics": {
      "major": 0.4059944016063447,
      "minor": {
        "em": 0.155299055613851,
        "f1": 0.6566897475988385
      }
    }
  }
}
